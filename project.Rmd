---
title: "project"
author: ""
date: "2025-11-25"
output: html_document
---

The dataset used in this project corresponds to the supplementary file **MOESM2 (ESM)** from the publication by Fan et al. (2018). It contains the raw GC-TOF-MS metabolomics data for 120 urine samples\
from healthy adults (60 males and 60 females), enabling independent preprocessing, exploratory analysis, feature selection, and classification.

# Import Library

```{r}
library(readxl)
library(tidyverse)
library(ggplot2)
library(ellipse)
library(caret)
library(pROC)
library(pls)
library(DescTools)
library(dplyr)
library(mixOmics)
```

# Import dataset

```{r}
raw_df <- read_excel("41598_2018_29592_MOESM2_ESM.xlsx")
```

```{r}
sex_labels <- unlist(raw_df[3, 17:136])
sex <- as.factor(sex_labels)

df <- read_excel("41598_2018_29592_MOESM2_ESM.xlsx", skip = 6)

glimpse(df)
```

```{r}
# júlia: i'm not sure what this chunk is for because X and pca are not defined before any it uses the raw df
sex_row <- as.character(raw_df[3, ])

sample_names_raw <- sex_row[sex_row %in% c("Male", "Female")]

sample_cols <- colnames(X)

sex <- factor(sample_names_raw[1:length(sample_cols)])

length(sex)

table(sex)
all(!is.na(sex))
identical(length(sex), nrow(pca$x))
```

# Cleaning and Data Transformation

```{r}
# organize names
# first df line has de colnames
colnames(df) <- df[1, ]

df <- df[-1, ]

# convert to numeric
for (i in 17:ncol(df)) {  # first 16 columns = metadata
  df[[i]] <- as.numeric(df[[i]])
}

X <- df[, 17:ncol(df)] 

Y <- sex

colnames(X) <- paste0("S", seq_len(ncol(X)))

# number of metabolites and samples
dim(X)

```

It matches with the article 120 samples (60 males + 60 females)

# Summary statistics

```{r}
summary(X)
```

```{r}
# Count total zero values
sum(X == 0, na.rm = TRUE)
```

```{r}
# Missing values check
sum(is.na(X))
```

# Is it the data Gaussian Distributed?

```{r}
par(mfrow=c(1,2))
hist(as.numeric(X[1,]), main="Raw Distribution", xlab="Intensity")
qqnorm(as.numeric(X[1,]), main="QQ Plot Raw")
qqline(as.numeric(X[1,]))
par(mfrow=c(1,1))
```

# Histogram inspection

To assess distributional properties, histogram inspection was performed. Raw metabolite intensities showed pronounced right-skewness, supporting the need for a log-transformation to improve normality and stabilize variance prior to multivariate analysis.

```{r}
feat1 <- as.numeric(X[[1]]) #S1
feat2 <- as.numeric(X[[2]]) #S2

par(mfrow = c(2,2))

# Raw intensity distributions
hist(feat1, main = "Raw Intensities - Feature 1", xlab = "", col = "lightgray")
hist(feat2, main = "Raw Intensities - Feature 2", xlab = "", col = "lightgray")

# After log10 transformation
hist(log10(feat1 + 1), main = "Log10 - Feature 1", xlab = "", col = "lightgray")
hist(log10(feat2 + 1), main = "Log10 - Feature 2", xlab = "", col = "lightgray")

par(mfrow = c(1,1))
```

The raw intensity histograms show strong right-skewness with extreme values. After log10 transformation, the distributions become more symmetric and less heteroscedastic, supporting the use of log-transformed and autoscaled data for PCA and PLS-DA.

# Non linear

```{r}
# Convert to standard data frame to avoid tibble recycling issues
X_nozero <- as.data.frame(X)

for(i in 1:nrow(X_nozero)){
  row_vals <- as.numeric(X_nozero[i, ])  # convert row to numeric vector

  # Safeguard: if the row is all zeros (rare but possible)
  if(all(row_vals == 0)){
    row_vals[row_vals == 0] <- 1  # placeholder before log10
  } else {
    min_val <- min(row_vals[row_vals > 0], na.rm = TRUE)
    row_vals[row_vals == 0] <- min_val / 2
  }

  X_nozero[i, ] <- row_vals  # assign back properly
}
```

# PCA

A preliminary variance-based feature filtering step (top 20% most variable metabolites) was evaluated as an unsupervised method for noise reduction and dimensionality control. However, this procedure resulted in the removal of several metabolites previously identified as statistically significant in the univariate analysis, consequently reducing the number of biologically relevant features and weakening the discrimination between sexes in downstream multivariate models. Therefore, variance filtering was not retained in the final workflow to preserve subtle yet meaningful metabolic differences.

The matrix was transposed so that samples correspond to rows and metabolites to columns, which is the correct structure required for multivariate analyses such as PCA and PLS-DA, where each row must represent an independent observation:

-   With \<- scale(t(X_log)).

```{r}
# Log10 transformation
X_log <- log10(X_nozero)

# Scaling for PCA (samples as rows)
X_scaled <- scale(t(X_log))

# PCA
pca <- prcomp(X_scaled, center = TRUE, scale. = FALSE)
summary(pca)

```

-   Summary statistics show very different scales and non-normal distributions

-   A few zero values detected → replaced with half of minimum value\
    → required for log10 transformation

-   Applied log10(x) to reduce skewness and stabilize variance

-   Autoscaling (mean-center + unit variance) to make metabolites comparable


```{r}
pca_scores <- as.data.frame(pca$x[, 1:2])


center <- colMeans(pca_scores)
cov_mat <- cov(pca_scores)

distances <- mahalanobis(pca_scores, center, cov_mat)

# Cutoff (chi with p=0.95 and 2 df)
cutoff <- qchisq(0.95, df = 2)

# plot with condidese matrix
plot(pca$x[,1], pca$x[,2], 
     pch = 19, 
     col = ifelse(distances > cutoff, "red", "black"), # Outliers a vermelho
     xlab = "PC1", ylab = "PC2", 
     main = "PCA Outlier Detection (95% CI)")

lines(ellipse(cov_mat, centre = center, level = 0.95), col="blue", lty=2)

text(pca$x[,1], pca$x[,2], 
     labels = ifelse(distances > cutoff, rownames(pca$x), ""), 
     pos = 3, cex = 0.7)

# Identify outliers
outliers <- which(distances > cutoff)
print(paste("Outliers detected:", length(outliers)))
```

We detected 9 outliers, but they weren't removed because:

Outliers reflect **real biological variability** within the healthy population.\
Removing them could introduce **bias**, especially if outliers are unevenly distributed between sexes.\
Furthermore, the original study **did not exclude any samples** all **120 subjects** were analyzed in full.

# PCA after log transformation

```{r}
score_df <- data.frame(
  PC1 = pca$x[,1],
  PC2 = pca$x[,2],
  Sex = sex
)

ggplot(score_df, aes(PC1, PC2, color = Sex)) +
  geom_point(size = 3) +
  theme_classic() +
  ggtitle("PCA After Log10 Transform + Scaling")


```

```{r}
# Scree plot (PCA Variance Explained)
variance <- pca$sdev^2 / sum(pca$sdev^2)

plot(variance[1:10], type="b", pch=19,
     xlab="PC", ylab="Proportion of Variance",
     main="Scree Plot - PCA")

```

-   Score plot shows a trend of separation between Male and Female

-   Some possible outliers appear (next step: detection)

-   Scree Plot indicates PC1+PC2 explain \~20–30% variance → normal for metabolomics

# Split Test/Training

We split the data into training and test sets (70/30) and perform univariate t-tests on the training set only. FDR correction is applied to identify metabolites significantly different between sexes.

```{r}
set.seed(123)

train_index <- createDataPartition(sex, p = 0.7, list = FALSE)

X_train <- X_scaled[train_index, ]
X_test  <- X_scaled[-train_index, ]

X_train <- as.data.frame(X_train)
X_test  <- as.data.frame(X_test)

y_train <- sex[train_index]
y_test  <- sex[-train_index]

pvals <- apply(X_train, 2, function(x) t.test(x ~ y_train)$p.value)
pvals_fdr <- p.adjust(pvals, method = "fdr")

head(sort(pvals_fdr))
```

Several metabolites show significant sex differences (FDR \< 0.05). These will be considered as candidates for biomarker selection in the supervised modeling step.

# Initials PLS-DA

An initial PLS-DA model with 10 components was trained on the training set to evaluate whether the metabolomic profiles discriminate between sexes.

```{r}
#Initial PLS-DA model
pls_initial <- plsda(X_train, y_train, ncomp = 10)

plotIndiv(pls_initial, comp = c(1,2),
          group = y_train, legend = TRUE,
          title = "Initial PLS-DA (Training Set)")
```

The PLS-DA score plot shows a clear separation between males and females along the first two latent variables, indicating strong sex-related differences in urinary metabolomic profiles.

# Internal Validation: 5-Fold CV + AUCROC

A 5-fold cross-validation was used on the training set to determine the optimal number of latent variables (LVs) for the PLS-DA model, using AUC as the performance metric.

```{r}

set.seed(123)

folds <- createFolds(y_train, k = 5, returnTrain = TRUE)
auc_results <- data.frame(LV = integer(), AUC = numeric())

for (lv in 1:5) {
  auc_fold <- c()
  
  for (f in folds) {
    model <- plsda(X_train[f,], y_train[f], ncomp = lv)
    pred <- predict(model, X_train[-f,])$predict[,1,lv]
    roc_obj <- roc(y_train[-f], pred)
    auc_fold <- c(auc_fold, auc(roc_obj))
  }
  
  auc_results <- rbind(auc_results,
                       data.frame(LV = lv,
                                  AUC = mean(auc_fold)))
}

auc_results

```

LV = 3 showed the highest mean AUC, and was therefore selected as the optimal number of components for the final PLS-DA model.

# Univariate Analysis on Training set (Male vs Female)

Univariate Welch t-tests were applied to each metabolite in the training set, followed by FDR correction. Mean differences (Male – Female) were calculated to determine the direction of change.

```{r}
# Compute raw p-values using Welch t-test for each metabolite
pvals <- apply(X_train, 2, function(x) t.test(x ~ y_train)$p.value)

# FDR correction (Benjamini-Hochberg)
pvals_fdr <- p.adjust(pvals, method = "fdr")


fc <- apply(X_train, 2, function(x) mean(x[y_train == "Male"]) -
                               mean(x[y_train == "Female"]))


uni_results <- data.frame(
  Metabolite = colnames(X_train),
  p_value = pvals,
  p_FDR = pvals_fdr,
  mean_diff = fc
)


uni_results <- uni_results[order(uni_results$p_FDR), ]
significant <- uni_results[uni_results$p_FDR < 0.05, ]
significant

```

Several metabolites showed significant sex differences (FDR \< 0.05). These features represent potential biomarkers and will be further evaluated in the supervised model.

# Volcano Plot - Univariate Analysis

A volcano plot was generated to visualize effect size (mean difference) against statistical significance (FDR-corrected p-values).

```{r}
library(ggplot2)

volcano <- uni_results
volcano$logP <- -log10(volcano$p_FDR)

ggplot(volcano, aes(x = mean_diff, y = logP)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = -log10(0.05), col = "red", lty = 2) +
  theme_classic() +
  xlab("Mean Difference (Male – Female)") +
  ylab("-log10(FDR)") +
  ggtitle("Volcano Plot — Univariate Analysis (Training Set)")

```

Univariate analysis performed on the training set identified 9 metabolites with significant sex-related differences after FDR correction (p_FDR \< 0.05). Six metabolites (V22, V399, V84, V51, V291, V57) showed higher concentrations in females, while three metabolites (V10, V113, V250) were higher in males. The strongest discriminators were V22 and V399 (p_FDR \< 0.005).

```{r}
colnames(X_train) <- paste0("X", 1:ncol(X_train))
colnames(X_test)  <- paste0("X", 1:ncol(X_test))

y_train <- as.factor(y_train)
y_test  <- as.factor(y_test)
```

```{r}
set.seed(30)

perf_res <- perf(pls_initial,
                 validation = "Mfold",
                 folds = 5,
                 nrepeat = 5,
                 progressBar = FALSE)

plot(perf_res, sd = TRUE)

```

Calculate the optimal number of components, using other metrics, like BER. This shows that increasing the number of components doesn't always decrease the error.

As an additional exploratory validation, the perf() method from mixOmics was applied. Although BER suggested 4 components, this criterion is less appropriate for binary classification than AUC. Therefore, AUC-based tuning remained the primary selection method.

```{r}
optimal_ncomp <- which.min(perf_res$error.rate$BER)
optimal_ncomp
```

```{r}
optimal_ncomp <-3
pls_final <- plsda(X_train, y_train, ncomp = optimal_ncomp)

plotIndiv(pls_final, comp = c(1,2),
          group = y_train, legend = TRUE,
          title = "Final PLS-DA (Training Set) — 3 components")
```

We selected n = 3 components because they maximised the AUC during cross-validation while avoiding unnecessary model complexity. Although BER suggested 4 components, the improvement was marginal and adding extra components risks overfitting, especially in a two-class dataset.

```{r}
pred_test <- predict(pls_final, X_test)
pred_class <- pred_test$class$max.dist[, optimal_ncomp]

table(True = y_test, Predicted = pred_class)
mean(pred_class == y_test)
```

```{r}
test_variates <- pred_test$variates

test_df <- data.frame(
  Comp1 = test_variates[,1],
  Comp2 = if (optimal_ncomp >= 2) test_variates[,2] else rep(0, nrow(test_variates)),
  Class = y_test
)

ggplot(test_df, aes(Comp1, Comp2, color = Class)) +
  geom_point(size = 3) +
  theme_minimal() +
  ggtitle("PLS-DA Test Set Scores")
```

The final PLS-DA model does not show a visibly stronger separation because the main class structure was already captured by the first two components in the initial model. Tuning improves model stability and classification performance, but these gains occur mostly beyond component 2, which is why the 2D score plot appears similar.

# Feature Selection: Calculate and Rank VIP Scores

# Calculate and Rank VIP Scores
```{r vip_ranking}
# 1. Extract the VIP scores matrix from the final PLS-DA model
# We typically focus on the scores from the first component (LV1)
vip_matrix <- vip(pls_final)
vip_scores_lv1 <- vip_matrix[, 1] 

# 2. Order the metabolites from most important to least important
vip_ranked <- sort(vip_scores_lv1, decreasing = TRUE)

# 3. Convert to a data frame for plotting and analysis
vip_df <- data.frame(
  Metabolite = names(vip_ranked),
  VIP = vip_ranked
)

# 4. Visualize the top 20 most important metabolites
top_n <- 20
vip_top <- head(vip_df, top_n)

# Generate the bar plot
library(ggplot2)
ggplot(vip_top, aes(x = reorder(Metabolite, VIP), y = VIP)) +
  geom_bar(stat = "identity", fill = "light blue") + 
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", size = 0.8) +
  coord_flip() + 
  theme_minimal(base_size = 14) +
  labs(title = paste("Top", top_n, "Metabolites by VIP Score (LV1)"),
       y = "VIP Score",
       x = "Metabolite")
```

# Implement Recursive Feature Elimination (RFE) using the k-NN Classifier

júlia -> no he pogut fer que funcioni, tot i aixi he fet el següent pas (NO HO EXECUTEU)

```{r rfe_knn_execution}
# 1. Define the control parameters for RFE
# We use 5-fold Cross-Validation (CV) repeated 5 times, as often done in RFE.
rfe_control <- rfeControl(
  functions = caretFuncs, # Use standard caret functions (for k-NN)
  method = "repeatedcv",
  number = 5,
  repeats = 5,
  verbose = FALSE # Set to TRUE for detailed output during execution
)

# 2. Define the sizes of the feature subsets to evaluate (The 'keepX' equivalent)
# This tests subset sizes from 2 to 50 features.
subset_sizes <- c(2, 5, 10, 15, 20, 25, 30, 40, 50) 

# 3. Create a feature ranking function based on PLS-DA VIP scores
# This function is crucial: RFE uses this to decide which features to eliminate.
# We are overriding the default ranking (which is usually a t-test or linear model)
# to use the more informed VIP scores from our PLS-DA analysis.

# We must first ensure 'vip_ranked' from the previous step is available:
if (!exists("vip_ranked")) {
  stop("Error: 'vip_ranked' is missing. Please run the VIP ranking code block first.")
}

# Now, define the custom RFE functions using the VIP ranking
vip_rfe_func <- caretFuncs
vip_rfe_func$selectSize <- pickSizeBest
vip_rfe_func$rank <- function(X, y) {
  # This function just returns the names of the features in the order of the pre-calculated VIP ranking.
  # It ensures RFE eliminates features based on VIP and not on a default method.
  
  # Ensure the ranking only includes features actually present in X
  current_features <- names(X)
  vip_subset <- vip_ranked[names(vip_ranked) %in% current_features]
  
  # Return the names, which RFE will use to eliminate features sequentially
  return(names(vip_subset))
}

# 4. Execute the RFE
# We use k-NN as the classifier, matching the lab guide.
rfe_results <- rfe(x = X_train,
                   y = y_train,
                   sizes = subset_sizes,
                   rfeControl = rfe_control,
                   method = "knn",
                   tuneLength = 5, # Test 5 different 'k' values for k-NN
                   metric = "Accuracy", # Use Accuracy as the metric to choose the best model
                   preProcess = c("center", "scale"), # Standardize data within CV folds
                   functions = vip_rfe_func) # Use our custom VIP-based ranking

# 5. Extract the optimal number of features
optimal.nfeatures.rfe <- rfe_results$bestSubset

cat("Optimal number of features found by RFE:", optimal.nfeatures.rfe, "\n")
cat("Top 10 selected features:", head(rfe_results$optVariables, 10), "\n")

# Optional: Plot the RFE results
plot(rfe_results, type = c("g", "o"), main = "RFE Performance Curve (k-NN, VIP-Ranked)")

# 6. Save the list of optimal features
optimal_features_rfe <- rfe_results$optVariables

# Ensure the final model uses the correct training data (only optimal features)
X_train_optimal <- X_train[, optimal_features_rfe]
```


# Final Model Training and Evaluation with Optimal Features
```{r}
# 1. Define the number of components (LVs)
# For simplicity, we use the previously optimized ncomp = 3.
optimal_ncomp <- 3 

# 2. Train the final PLS-DA model using ONLY the optimal features
# We use the full original training data, but only the selected columns.
pls_final_optimal <- plsda(X_train_optimal, y_train, ncomp = optimal_ncomp)

cat("Final PLS-DA model trained with:", optimal_nfeatures.rfe, "metabolites.\n")

# 3. Prepare the Test Data
# IMPORTANT: The test set (X_test) must be filtered to include only the optimal features.
# Use the list of features obtained from RFE (optimal_features_rfe)
X_test_optimal <- X_test[, optimal_features_rfe]

# 4. Predict on the Test Set
pred_test_optimal <- predict(pls_final_optimal, X_test_optimal)

# Extract the predicted class using the max distance for the optimal ncomp
pred_class_optimal <- pred_test_optimal$class$max.dist[, optimal_ncomp]

# 5. Evaluate Performance
cat("\n--- Performance on Test Set ---\n")

# Confusion Matrix
confusion_matrix_optimal <- table(True = y_test, Predicted = pred_class_optimal)
print(confusion_matrix_optimal)

# Overall Accuracy
accuracy_optimal <- mean(pred_class_optimal == y_test)
cat("\nOverall Accuracy:", round(accuracy_optimal, 4), "\n")

# Balanced Error Rate (BER) - Better for two-class problems
ber_optimal <- BER(confusion_matrix_optimal)
cat("Balanced Error Rate (BER):", round(ber_optimal, 4), "\n")
```

# Visualization of Test Set Scores
```{r plot_optimal_test}
# Extract the sample scores (variates) for the optimal model
test_variates_optimal <- pred_test_optimal$variates

# Create a data frame for plotting
test_df_optimal <- data.frame(
  Comp1 = test_variates_optimal[,1],
  Comp2 = if (optimal_ncomp >= 2) test_variates_optimal[,2] else rep(0, nrow(test_variates_optimal)),
  Class = y_test
)

# Generate the Score Plot
ggplot(test_df_optimal, aes(Comp1, Comp2, color = Class)) +
  geom_point(size = 3) +
  theme_minimal(base_size = 14) +
  scale_color_manual(values = c("Female" = "red", "Male" = "blue")) +
  ggtitle(paste("Final PLS-DA Test Set Scores (Features:", optimal_nfeatures.rfe, ")")) +
  labs(x = "Component 1", y = "Component 2")
```

Did the accuracy (accuracy_optimal) improved in relation to the original PLS-DA model that used all 407 metabolites?
- 

next step - Validate the results in external validation

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

